{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82db2d2b",
   "metadata": {},
   "source": [
    "# 프로젝트 : 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1ce5e",
   "metadata": {},
   "source": [
    "### 라이브러리 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd9c31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac4d671",
   "metadata": {},
   "source": [
    "### Step 1. 데이터 다운로드\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e97dfe4",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "184d331d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\", 'It goes like this', 'The fourth, the fifth', 'The minor fall, the major lift', 'The baffled king composing Hallelujah Hallelujah', 'Hallelujah', 'Hallelujah', 'Hallelujah Your faith was strong but you needed proof']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path) \n",
    "\n",
    "raw_corpus = [] \n",
    "\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines() \n",
    "        raw_corpus.extend(raw) \n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563a6bd9",
   "metadata": {},
   "source": [
    "### Step 3. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8033ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a416eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>',\n",
       " '<start> you saw her bathing on the roof <end>',\n",
       " '<start> her beauty and the moonlight overthrew her <end>',\n",
       " '<start> she tied you <end>',\n",
       " '<start> to a kitchen chair <end>',\n",
       " '<start> she broke your throne , and she cut your hair <end>',\n",
       " '<start> and from your lips she drew the hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah you say i took the name in vain <end>',\n",
       " '<start> i don t even know the name <end>',\n",
       " '<start> there s a blaze of light <end>',\n",
       " '<start> in every word <end>',\n",
       " '<start> it doesn t matter which you heard <end>',\n",
       " '<start> the holy or the broken hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah i did my best , it wasn t much <end>',\n",
       " '<start> i couldn t feel , so i tried to touch <end>',\n",
       " '<start> i ve told the truth , i didn t come to fool you <end>',\n",
       " '<start> and even though <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장\n",
    "corpus = []\n",
    "\n",
    "# raw_corpus list에 저장된 문장들을 순서대로 반환하여 sentence에 저장\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뛰기\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    # 구현한 preprocess_sentence() 함수를 이용하여 문장을 정제를 하고 담기\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    if len(preprocessed_sentence.split(' ')) <=15 :\n",
    "        corpus.append(preprocessed_sentence)\n",
    "    \n",
    "# 정제된 결과를 30개 확인\n",
    "corpus[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94f9256",
   "metadata": {},
   "source": [
    "preprocessed_sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    if len(preprocessed_sentence.split(' ')) <=15 :\n",
    "    \n",
    "        corpus.append(preprocessed_sentence)\n",
    "        \n",
    "--> \n",
    "\n",
    "토큰화 했을때 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외시키고 \n",
    "\n",
    "corpus에 append 하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78f8b457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156013"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60eb7532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(corpus[0].split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c827ffb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4 ...    0    0    0]\n",
      " [   2   15 2967 ...    0    0    0]\n",
      " [   2   33    7 ...   46    3    0]\n",
      " ...\n",
      " [   2    4  118 ...    0    0    0]\n",
      " [   2  258  194 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f9748527490>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용\n",
    "def tokenize(corpus):\n",
    "    # 12000단어를 기억할 수 있는 tokenizer\n",
    "    # 우리는 이미 문장을 정제했으니 filters가 필요없다\n",
    "    # 12000단어에 포함되지 못한 단어는 '<unk>'로 바꾼다\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "   \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1b33656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4   95  303   62   53    9  946 6263]\n",
      " [   2   15 2967  871    5    8   11 5739    6  374]\n",
      " [   2   33    7   40   16  164  288   28  333    5]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10]) # 행 , 렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5469869d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a70b90f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    4   95  303   62   53    9  946 6263    3    0    0    0]\n",
      "[  50    4   95  303   62   53    9  946 6263    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da71943e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156013\n",
      "156013\n"
     ]
    }
   ],
   "source": [
    "print(len(src_input))\n",
    "print(len(tgt_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794459b4",
   "metadata": {},
   "source": [
    "### Step 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "831f0582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                          tgt_input, \n",
    "                                                          test_size = 0.2, \n",
    "                                                          shuffle=True, \n",
    "                                                          random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94a7c714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124810\n",
      "31203\n",
      "124810\n",
      "31203\n"
     ]
    }
   ],
   "source": [
    "# print(enc_train)\n",
    "# print(enc_val)\n",
    "# print(dec_train)\n",
    "# print(dec_val)\n",
    "\n",
    "print(len(enc_train))\n",
    "print(len(enc_val))\n",
    "print(len(dec_train))\n",
    "print(len(dec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a53665c",
   "metadata": {},
   "source": [
    "### Step 5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "151668ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만든다\n",
    "# 데이터셋에 대해서는 아래 문서를 참고\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de742a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)  \n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "embedding_size = 256 \n",
    "hidden_size = 1024 \n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d183f02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-2.50610785e-04,  2.28238685e-04, -1.25838385e-04, ...,\n",
       "          9.10183226e-05,  9.41522812e-05, -2.96658545e-04],\n",
       "        [-2.45328672e-04,  2.34896026e-04, -2.62794696e-04, ...,\n",
       "         -8.40862558e-05,  2.67433468e-04, -2.36708875e-04],\n",
       "        [-2.37111817e-04,  2.86232244e-04, -1.62003314e-04, ...,\n",
       "         -2.66933261e-04,  3.48057074e-04,  9.48211600e-05],\n",
       "        ...,\n",
       "        [-2.50884459e-05, -8.12940300e-04, -7.24632002e-04, ...,\n",
       "         -4.14517883e-04,  9.09898139e-04,  1.03275140e-03],\n",
       "        [ 1.89090370e-05, -1.17992004e-03, -8.29560391e-04, ...,\n",
       "         -4.60609124e-04,  1.16841902e-03,  1.09895505e-03],\n",
       "        [ 7.41773329e-05, -1.50798017e-03, -9.21876170e-04, ...,\n",
       "         -4.99588263e-04,  1.38547854e-03,  1.16246182e-03]],\n",
       "\n",
       "       [[-2.50610785e-04,  2.28238685e-04, -1.25838385e-04, ...,\n",
       "          9.10183226e-05,  9.41522812e-05, -2.96658545e-04],\n",
       "        [-2.35149113e-04,  4.49794024e-04, -2.13834108e-04, ...,\n",
       "          7.65801815e-05,  7.55500441e-05, -2.07550504e-04],\n",
       "        [-4.27922467e-04,  5.14733838e-04, -3.14295525e-04, ...,\n",
       "          1.33951053e-05, -2.81049961e-05, -2.07668709e-04],\n",
       "        ...,\n",
       "        [ 8.11849488e-04, -1.18039490e-03, -9.37302364e-04, ...,\n",
       "         -8.86431371e-04,  1.15733512e-03, -1.03223683e-04],\n",
       "        [ 7.97236105e-04, -1.44897774e-03, -9.79530276e-04, ...,\n",
       "         -8.66479881e-04,  1.39601855e-03,  1.19502845e-04],\n",
       "        [ 7.70712155e-04, -1.69571675e-03, -1.01789925e-03, ...,\n",
       "         -8.46463139e-04,  1.58314477e-03,  3.30103707e-04]],\n",
       "\n",
       "       [[-2.50610785e-04,  2.28238685e-04, -1.25838385e-04, ...,\n",
       "          9.10183226e-05,  9.41522812e-05, -2.96658545e-04],\n",
       "        [-1.10642177e-04,  5.53928607e-04, -8.03045841e-05, ...,\n",
       "          1.04212311e-04,  9.48085435e-05, -5.32560691e-04],\n",
       "        [-8.18057451e-05,  5.86097711e-04, -1.06397551e-04, ...,\n",
       "          2.65243492e-04, -1.16792689e-04, -3.65878834e-04],\n",
       "        ...,\n",
       "        [ 6.25295914e-04, -1.66887918e-03, -8.35612242e-04, ...,\n",
       "         -8.15426640e-04,  1.16956118e-03,  4.35808644e-04],\n",
       "        [ 6.11252035e-04, -1.93202728e-03, -9.25536035e-04, ...,\n",
       "         -8.20503861e-04,  1.36008661e-03,  5.87695336e-04],\n",
       "        [ 5.91909979e-04, -2.15349253e-03, -1.00394653e-03, ...,\n",
       "         -8.18239758e-04,  1.51230791e-03,  7.31830369e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.50610785e-04,  2.28238685e-04, -1.25838385e-04, ...,\n",
       "          9.10183226e-05,  9.41522812e-05, -2.96658545e-04],\n",
       "        [-5.77793864e-04,  7.19800708e-04, -1.33961337e-04, ...,\n",
       "          3.19025072e-04,  1.32508285e-04, -4.32713277e-04],\n",
       "        [-4.07991931e-04,  1.20441685e-03, -7.87086683e-05, ...,\n",
       "          2.73093930e-04,  2.03220217e-04, -5.23592578e-04],\n",
       "        ...,\n",
       "        [-1.05659559e-03,  1.08090183e-03,  5.65404596e-04, ...,\n",
       "         -4.83499956e-04, -8.35512401e-05,  8.71231663e-04],\n",
       "        [-8.24174203e-04,  9.21292114e-04,  6.29445887e-04, ...,\n",
       "         -7.17216928e-04,  2.66993393e-05,  7.48529506e-04],\n",
       "        [-6.34230906e-04,  6.27273927e-04,  7.34010478e-04, ...,\n",
       "         -8.83506378e-04,  1.82548305e-04,  7.03446625e-04]],\n",
       "\n",
       "       [[-2.50610785e-04,  2.28238685e-04, -1.25838385e-04, ...,\n",
       "          9.10183226e-05,  9.41522812e-05, -2.96658545e-04],\n",
       "        [-6.01510576e-04,  2.10910875e-04, -2.21002207e-04, ...,\n",
       "          2.67104944e-04, -8.23956798e-05, -3.78448603e-04],\n",
       "        [-5.59284876e-04,  3.46844230e-04, -1.60246025e-04, ...,\n",
       "          9.66645257e-06, -3.85998079e-04, -1.49166066e-04],\n",
       "        ...,\n",
       "        [ 3.74991068e-04, -1.71329780e-03, -5.17301902e-04, ...,\n",
       "         -8.83744040e-04,  1.08705310e-03,  5.69600670e-04],\n",
       "        [ 4.16167924e-04, -1.95997977e-03, -6.54155854e-04, ...,\n",
       "         -8.29219120e-04,  1.30138011e-03,  6.81207574e-04],\n",
       "        [ 4.43084282e-04, -2.16988451e-03, -7.80080329e-04, ...,\n",
       "         -7.84145319e-04,  1.47210271e-03,  7.97727495e-04]],\n",
       "\n",
       "       [[-2.50610785e-04,  2.28238685e-04, -1.25838385e-04, ...,\n",
       "          9.10183226e-05,  9.41522812e-05, -2.96658545e-04],\n",
       "        [-1.76009940e-04,  4.73451451e-04, -1.81529031e-04, ...,\n",
       "         -1.13605629e-05,  4.99852118e-04, -4.52951150e-04],\n",
       "        [ 5.11430626e-05,  5.94556681e-04, -3.60027567e-04, ...,\n",
       "          3.64547086e-05,  6.34091790e-04, -5.63159760e-04],\n",
       "        ...,\n",
       "        [ 1.02550734e-03, -9.35648859e-04, -1.23084185e-03, ...,\n",
       "         -6.27648318e-04,  1.69380242e-03,  8.14412269e-05],\n",
       "        [ 1.00127340e-03, -1.31881540e-03, -1.25744345e-03, ...,\n",
       "         -7.31726177e-04,  1.86414993e-03,  2.52808764e-04],\n",
       "        [ 9.59437864e-04, -1.64711534e-03, -1.27541902e-03, ...,\n",
       "         -8.03564326e-04,  1.98072218e-03,  4.20362485e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46f55391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델의 구조를 확인\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aadfc907",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam() # Adam은 현재 가장 많이 사용하는 옵티마이저\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy( # 훈련 데이터의 라벨이 정수의 형태로 제공될 때 사용하는 손실함수\n",
    "    from_logits=True,\n",
    "    reduction='none' \n",
    ")\n",
    "# 모델을 학습시키키 위한 학습과정을 설정하는 단계\n",
    "model.compile(loss=loss, optimizer=optimizer) # 손실함수와 훈련과정을 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6755650d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "609/609 [==============================] - 96s 150ms/step - loss: 3.4127\n",
      "Epoch 2/30\n",
      "609/609 [==============================] - 99s 162ms/step - loss: 2.9593\n",
      "Epoch 3/30\n",
      "609/609 [==============================] - 103s 168ms/step - loss: 2.7931\n",
      "Epoch 4/30\n",
      "609/609 [==============================] - 106s 173ms/step - loss: 2.6636\n",
      "Epoch 5/30\n",
      "609/609 [==============================] - 106s 173ms/step - loss: 2.5517\n",
      "Epoch 6/30\n",
      "609/609 [==============================] - 106s 173ms/step - loss: 2.4505\n",
      "Epoch 7/30\n",
      "609/609 [==============================] - 106s 173ms/step - loss: 2.3572\n",
      "Epoch 8/30\n",
      "609/609 [==============================] - 106s 173ms/step - loss: 2.2703\n",
      "Epoch 9/30\n",
      "609/609 [==============================] - 106s 173ms/step - loss: 2.1895\n",
      "Epoch 10/30\n",
      "609/609 [==============================] - 105s 173ms/step - loss: 2.1135\n",
      "Epoch 11/30\n",
      "609/609 [==============================] - 105s 173ms/step - loss: 2.0412\n",
      "Epoch 12/30\n",
      "609/609 [==============================] - 105s 173ms/step - loss: 1.9724\n",
      "Epoch 13/30\n",
      "609/609 [==============================] - 105s 173ms/step - loss: 1.9068\n",
      "Epoch 14/30\n",
      "609/609 [==============================] - 105s 173ms/step - loss: 1.8438\n",
      "Epoch 15/30\n",
      "609/609 [==============================] - 106s 173ms/step - loss: 1.7832\n",
      "Epoch 16/30\n",
      "609/609 [==============================] - 106s 173ms/step - loss: 1.7252\n",
      "Epoch 17/30\n",
      "609/609 [==============================] - 106s 173ms/step - loss: 1.6694\n",
      "Epoch 18/30\n",
      "609/609 [==============================] - 106s 174ms/step - loss: 1.6161\n",
      "Epoch 19/30\n",
      "609/609 [==============================] - 105s 173ms/step - loss: 1.5654\n",
      "Epoch 20/30\n",
      "609/609 [==============================] - 106s 173ms/step - loss: 1.5166\n",
      "Epoch 21/30\n",
      "609/609 [==============================] - 105s 173ms/step - loss: 1.4705\n",
      "Epoch 22/30\n",
      "609/609 [==============================] - 105s 173ms/step - loss: 1.4264\n",
      "Epoch 23/30\n",
      "609/609 [==============================] - 105s 173ms/step - loss: 1.3852\n",
      "Epoch 24/30\n",
      "609/609 [==============================] - 105s 173ms/step - loss: 1.3456\n",
      "Epoch 25/30\n",
      "609/609 [==============================] - 105s 173ms/step - loss: 1.3087\n",
      "Epoch 26/30\n",
      "609/609 [==============================] - 105s 173ms/step - loss: 1.2742\n",
      "Epoch 27/30\n",
      "609/609 [==============================] - 105s 173ms/step - loss: 1.2418\n",
      "Epoch 28/30\n",
      "609/609 [==============================] - 105s 172ms/step - loss: 1.2113\n",
      "Epoch 29/30\n",
      "609/609 [==============================] - 105s 173ms/step - loss: 1.1830\n",
      "Epoch 30/30\n",
      "609/609 [==============================] - 105s 172ms/step - loss: 1.1565\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataset, epochs=30)\n",
    "#Loss\n",
    "# tf.keras.losses.SparseCategoricalCrossentropy : https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy( \n",
    "    from_logits=True, reduction='none') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f30f5160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장생성 함수 정의\n",
    "#모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20): \n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence]) \n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다 (도달 하지 못하였으면 while 루프를 돌면서 다음 단어를 예측)\n",
    "    while True: #루프를 돌면서 init_sentence에 단어를 하나씩 생성\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4 \n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated #최종적으로 모델이 생성한 문장을 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8090e87e",
   "metadata": {},
   "source": [
    "# 훈련한 모델로 다양한 가사 만들어보기-!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d63705",
   "metadata": {},
   "source": [
    "##### 시작문장으로 i love 를 넣어 문장생성 함수 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f7034822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i m not gonna crack <end> '"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdbeef0",
   "metadata": {},
   "source": [
    "##### 시작문장으로 i 를 넣어 문장생성 함수 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2abc6465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m gonna be a little selfish <end> '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i\", max_len=20) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0263d7",
   "metadata": {},
   "source": [
    "##### 시작문장으로 i am 을 넣어 문장생성 함수 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34cc63a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i am not throwing away my shot <end> '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i am\", max_len=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e74586",
   "metadata": {},
   "source": [
    "##### 시작문장으로 i like 를 넣어 문장생성 함수 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dcbe85e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i like it when you tro it pon me <end> '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i like\", max_len=20) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d827710c",
   "metadata": {},
   "source": [
    "##### 시작문장으로 my 를 넣어 문장생성 함수 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e70066f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> my name is who ? <end> '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> my\", max_len=40) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b73b93",
   "metadata": {},
   "source": [
    "##### 시작문장으로 you 를 넣어 문장생성 함수 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "387d7a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you know i m bad , i m bad you know it <end> '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you\", max_len=30) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc43e33",
   "metadata": {},
   "source": [
    "##### 시작문장으로 my love 를 넣어 문장생성 함수 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "450eaed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> my love for you is eternal one <end> '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> my love\", max_len=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa55458",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d94a2d",
   "metadata": {},
   "source": [
    "- 이번 프로젝트에서 **어려웠던 점,**\n",
    "\n",
    "이전 fund에서 NLP에대해 처음접해보았는데, 개념부터 너무 어려웠었다.\n",
    "\n",
    "model fit을 하는데 시간이 너무 오래걸려서 힘들었다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c30fd8",
   "metadata": {},
   "source": [
    "- 프로젝트를 진행하면서 **알아낸 점** 혹은 **아직 모호한 점**.\n",
    "\n",
    "함수와 코드를 하나하나보면서 이해했어야했는데, 그러지 못한 상태로 마무리를 하게되어 아쉽다.\n",
    "\n",
    "자연어 처리를 위해서 정규표현식을 했는데, 정규표현식을 좀 더 공부해야겠다.\n",
    "\n",
    "옵티마이저로 Adam을 사용했는데, 다른 옵티마이저를 사용하면 어떻게 될지 궁금해졌다.\n",
    "\n",
    "하이퍼파라미터의 값을 바꾸며 학습을 시켜보았을땐 어떨지 도전해봐야겠다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac9efec",
   "metadata": {},
   "source": [
    "- 루브릭 평가 지표를 맞추기 위해 **시도한 것들**.\n",
    "\n",
    "특수문자 제거, 토크나이저 생성, 토큰화 했을때 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외시키기, \n",
    "단어장크기 12,000장 이상, 평가데이터셋 총 데이터의 20%, \n",
    "\n",
    "10 Epoch 안에 val_loss값 2.2 줄여보기\n",
    "\n",
    "Epoch 10/30\n",
    "609/609 [==============================] - 105s 173ms/step - loss: 2.1135\n",
    "\n",
    "텍스트 제너레이션 결과로 생성된 문장이 해석가능한 문장인지 살펴보기\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97261c98",
   "metadata": {},
   "source": [
    "- 만약에 루브릭 평가 관련 지표를 **달성 하지 못했을 때, 이유에 관한 추정.느낌\n",
    "\n",
    "이전 노드의 내용을 그대로 참고해 학습부분에서 스스로 고민해본것이 부족했다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a2e0fb",
   "metadata": {},
   "source": [
    "- **자기 다짐**\n",
    "\n",
    "관심있는 분야와 그렇지 않은 분야를 정해놓고 어느 한쪽에 집중하는건 맞지만 \n",
    "\n",
    "학습에서도 소홀히 하면 결국 나에게 좋을건 없다는걸 다시한번 더 깨닳았다.\n",
    "\n",
    "Embedding 개념을 다시 한번 살펴보고 이번 프로젝트와 19번 fund의 내용을 익힐 수 있게 노력해야겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421cc424",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
