{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37266d6",
   "metadata": {},
   "source": [
    "# 프로젝트 : 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c3ebd4",
   "metadata": {},
   "source": [
    "### 라이브러리 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69c7983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import glob  #glob 모듈의 glob 함수는 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환한다\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057e139",
   "metadata": {},
   "source": [
    "### Step 1. 데이터 다운로드\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbd3126",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1683dedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\", 'It goes like this', 'The fourth, the fifth', 'The minor fall, the major lift', 'The baffled king composing Hallelujah Hallelujah', 'Hallelujah', 'Hallelujah', 'Hallelujah Your faith was strong but you needed proof']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*' #os.getenv(x)함수는 환경 변수x의 값을 포함하는 문자열 변수를 반환합니다. txt_file_path 에 \"/root/aiffel/lyricist/data/lyrics/*\" 저장\n",
    "\n",
    "txt_list = glob.glob(txt_file_path) #txt_file_path 경로에 있는 모든 파일명을 리스트 형식으로 txt_list 에 할당\n",
    "\n",
    "raw_corpus = [] \n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines() #read() : 파일 전체의 내용을 하나의 문자열로 읽어온다. , splitlines()  : 여러라인으로 구분되어 있는 문자열을 한라인씩 분리하여 리스트로 반환\n",
    "        raw_corpus.extend(raw) # extend() : 리스트함수로 추가적인 내용을 연장 한다.\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c9cadb",
   "metadata": {},
   "source": [
    "### Step 3. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f4b5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1655ef2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>',\n",
       " '<start> you saw her bathing on the roof <end>',\n",
       " '<start> her beauty and the moonlight overthrew her <end>',\n",
       " '<start> she tied you <end>',\n",
       " '<start> to a kitchen chair <end>',\n",
       " '<start> she broke your throne , and she cut your hair <end>',\n",
       " '<start> and from your lips she drew the hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah you say i took the name in vain <end>',\n",
       " '<start> i don t even know the name <end>',\n",
       " '<start> there s a blaze of light <end>',\n",
       " '<start> in every word <end>',\n",
       " '<start> it doesn t matter which you heard <end>',\n",
       " '<start> the holy or the broken hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah i did my best , it wasn t much <end>',\n",
       " '<start> i couldn t feel , so i tried to touch <end>',\n",
       " '<start> i ve told the truth , i didn t come to fool you <end>',\n",
       " '<start> and even though <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장\n",
    "corpus = []\n",
    "\n",
    "# raw_corpus list에 저장된 문장들을 순서대로 반환하여 sentence에 저장\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뛰기\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    # 앞서 구현한 preprocess_sentence() 함수를 이용하여 문장을 정제를 하고 담기\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    if len(preprocessed_sentence.split(' ')) <=15 :\n",
    "        corpus.append(preprocessed_sentence)\n",
    "    \n",
    "# 정제된 결과를 30개 확인\n",
    "corpus[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db42de04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156013"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e030619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(corpus[0].split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d991e188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4 ...    0    0    0]\n",
      " [   2   15 2967 ...    0    0    0]\n",
      " [   2   33    7 ...   46    3    0]\n",
      " ...\n",
      " [   2    4  118 ...    0    0    0]\n",
      " [   2  258  194 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f2e3811bee0>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용\n",
    "def tokenize(corpus):\n",
    "    # 12000단어를 기억할 수 있는 tokenizer\n",
    "    # 우리는 이미 문장을 정제했으니 filters가 필요없다\n",
    "    # 12000단어에 포함되지 못한 단어는 '<unk>'로 바꾼다\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성한다\n",
    "    # tokenizer.fit_on_texts(texts): 문자 데이터를 입력받아 리스트의 형태로 변환하는 메서드\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환한다\n",
    "    # tokenizer.texts_to_sequences(texts): 텍스트 안의 단어들을 숫자의 시퀀스 형태로 변환하는 메서드\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰주기\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰주기\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b806fe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4   95  303   62   53    9  946 6263]\n",
      " [   2   15 2967  871    5    8   11 5739    6  374]\n",
      " [   2   33    7   40   16  164  288   28  333    5]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10]) # 행 , 렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6ce4593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# tokenizer.index_word: 현재 계산된 단어의 인덱스와 인덱스에 해당하는 단어를 dictionary 형대로 반환 (Ex. {index: '~~', index: '~~', ...})\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91205e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    4   95  303   62   53    9  946 6263    3    0    0    0]\n",
      "[  50    4   95  303   62   53    9  946 6263    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높다\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])\n",
    "\n",
    "# 행 뒤쪽에 0이 많은 나온 부분은 정해진 입력 시퀀스 길이보다 문장이 짧을 경우 \n",
    "# 0으로 패딩(padding)을 채워 넣은 것\n",
    "# 0은 바로 패딩 문자 <pad>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d887e6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156013\n",
      "156013\n"
     ]
    }
   ],
   "source": [
    "print(len(src_input))\n",
    "print(len(tgt_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8369aef9",
   "metadata": {},
   "source": [
    "### Step 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c457278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                          tgt_input, \n",
    "                                                          test_size = 0.2, \n",
    "                                                          shuffle=True, \n",
    "                                                          random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "566b43f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124810\n",
      "31203\n",
      "124810\n",
      "31203\n"
     ]
    }
   ],
   "source": [
    "# print(enc_train)\n",
    "# print(enc_val)\n",
    "# print(dec_train)\n",
    "# print(dec_val)\n",
    "\n",
    "print(len(enc_train))\n",
    "print(len(enc_val))\n",
    "print(len(dec_train))\n",
    "print(len(dec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6092f61",
   "metadata": {},
   "source": [
    "### Step 5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8933c6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    " # tokenizer.num_words: 주어진 데이터의 문장들에서 빈도수가 높은 n개의 단어만 선택\n",
    " # tokenize() 함수에서 num_words를 7000개로 선언했기 때문에, tokenizer.num_words의 값은 7000\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만든다\n",
    "# 데이터셋에 대해서는 아래 문서를 참고\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b53ea01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성되어 있다\n",
    "        # Embedding 레이어는 단어 사전의 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔준다\n",
    "        # 이 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현으로 사용\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)  \n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "# embedding size 값이 커질수록 단어의 추상적인 특징들을 더 잡아낼 수 있지만\n",
    "# 그만큼 충분한 데이터가 없으면 안좋은 결과 값을 가져온다   \n",
    "embedding_size = 256 # 워드 벡터의 차원수를 말하며 단어가 추상적으로 표현되는 크기\n",
    "hidden_size = 1024 # 모델에 얼마나 많은 일꾼을 둘 것인가? 정도로 이해하면 좋다\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size) # tokenizer.num_words에 +1인 이유는 문장에 없는 pad가 사용되었기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b13b15ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-8.93012184e-05,  7.20549506e-05,  9.29457528e-05, ...,\n",
       "          2.51660647e-04, -7.69653052e-05, -5.14737367e-05],\n",
       "        [-7.50735635e-05, -2.91304255e-04,  1.68191458e-04, ...,\n",
       "          2.52558821e-04,  1.92126827e-04, -4.31698863e-04],\n",
       "        [-1.21092344e-04, -3.92713468e-04,  2.71301251e-04, ...,\n",
       "          8.04730880e-05,  4.00504650e-04, -4.79042967e-04],\n",
       "        ...,\n",
       "        [ 1.27847143e-03, -2.89400370e-04, -1.62393306e-04, ...,\n",
       "          6.40396902e-04,  1.37764553e-03,  7.25694757e-04],\n",
       "        [ 1.33159105e-03,  1.01393474e-04, -3.08291928e-04, ...,\n",
       "          7.24932994e-04,  1.71499804e-03,  9.37055796e-04],\n",
       "        [ 1.31831015e-03,  4.65723162e-04, -5.00603230e-04, ...,\n",
       "          1.00981188e-03,  1.51096901e-03,  1.24855794e-03]],\n",
       "\n",
       "       [[-8.93012184e-05,  7.20549506e-05,  9.29457528e-05, ...,\n",
       "          2.51660647e-04, -7.69653052e-05, -5.14737367e-05],\n",
       "        [-5.57078929e-05,  1.41050434e-04,  3.14553472e-04, ...,\n",
       "          2.69240991e-04, -6.15177196e-05, -2.92962271e-04],\n",
       "        [-4.42239107e-04,  1.56474271e-04,  4.78255592e-04, ...,\n",
       "          3.20153369e-04, -1.49050538e-05, -5.09913894e-04],\n",
       "        ...,\n",
       "        [-3.58931371e-04,  1.79820007e-03, -9.03932261e-04, ...,\n",
       "          2.89359619e-03, -1.40910433e-03,  1.13695755e-03],\n",
       "        [-5.40797424e-04,  1.98539533e-03, -1.21912512e-03, ...,\n",
       "          3.38875270e-03, -1.74307090e-03,  1.20011868e-03],\n",
       "        [-7.52311433e-04,  2.17746734e-03, -1.50238909e-03, ...,\n",
       "          3.84878065e-03, -2.02232087e-03,  1.20288949e-03]],\n",
       "\n",
       "       [[-8.93012184e-05,  7.20549506e-05,  9.29457528e-05, ...,\n",
       "          2.51660647e-04, -7.69653052e-05, -5.14737367e-05],\n",
       "        [-1.10223395e-04, -1.76786998e-05,  1.74087370e-04, ...,\n",
       "          2.52082187e-04, -1.28866333e-04, -5.24444877e-05],\n",
       "        [-2.16599205e-04, -1.75938869e-04,  2.15984051e-04, ...,\n",
       "          2.98658444e-04, -9.41748149e-05,  2.11463324e-04],\n",
       "        ...,\n",
       "        [-1.61217249e-04,  4.47816099e-04, -1.36767427e-04, ...,\n",
       "          4.14683396e-04,  1.09493406e-03,  6.27307047e-04],\n",
       "        [-1.71117772e-05,  7.94894819e-04, -4.69360908e-04, ...,\n",
       "          7.49161874e-04,  8.12321028e-04,  8.49161996e-04],\n",
       "        [ 8.94749392e-05,  1.05774286e-03, -7.78440211e-04, ...,\n",
       "          1.23255036e-03,  3.51381081e-04,  1.09478191e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-8.93012184e-05,  7.20549506e-05,  9.29457528e-05, ...,\n",
       "          2.51660647e-04, -7.69653052e-05, -5.14737367e-05],\n",
       "        [-2.49307574e-04,  2.15868451e-04,  1.99358430e-04, ...,\n",
       "          3.72472306e-04,  1.16387946e-05, -1.29167791e-04],\n",
       "        [-9.95951414e-05,  1.47216851e-04,  4.58225055e-04, ...,\n",
       "          2.55272695e-04, -8.43816961e-05, -5.02374896e-05],\n",
       "        ...,\n",
       "        [ 1.01428572e-03,  1.76603789e-04,  4.99542570e-04, ...,\n",
       "          8.70460819e-04,  3.06273432e-04,  1.42480154e-03],\n",
       "        [ 9.03189881e-04,  4.58654104e-04,  1.73422959e-04, ...,\n",
       "          1.50726002e-03, -9.90480839e-05,  1.64977065e-03],\n",
       "        [ 7.16233510e-04,  7.44464458e-04, -1.47702900e-04, ...,\n",
       "          2.14539142e-03, -5.17509412e-04,  1.77666801e-03]],\n",
       "\n",
       "       [[-8.93012184e-05,  7.20549506e-05,  9.29457528e-05, ...,\n",
       "          2.51660647e-04, -7.69653052e-05, -5.14737367e-05],\n",
       "        [-1.14086033e-04, -9.59875251e-05,  3.91545647e-04, ...,\n",
       "          1.59028830e-04, -1.68608924e-04,  2.77706840e-05],\n",
       "        [ 1.60626267e-04, -1.89197643e-04,  1.42831152e-04, ...,\n",
       "         -2.26462784e-04, -1.62177283e-04, -3.14538629e-05],\n",
       "        ...,\n",
       "        [-3.30044481e-04,  1.58027897e-03, -1.86136994e-03, ...,\n",
       "          1.87132892e-03, -5.41696674e-04,  7.66316080e-04],\n",
       "        [-4.84008749e-04,  1.84016000e-03, -1.95543445e-03, ...,\n",
       "          2.47382349e-03, -9.95816663e-04,  9.00321931e-04],\n",
       "        [-6.82984421e-04,  2.08998146e-03, -2.04112497e-03, ...,\n",
       "          3.04256054e-03, -1.38273521e-03,  9.68569366e-04]],\n",
       "\n",
       "       [[-8.93012184e-05,  7.20549506e-05,  9.29457528e-05, ...,\n",
       "          2.51660647e-04, -7.69653052e-05, -5.14737367e-05],\n",
       "        [ 1.28700514e-04,  5.62190944e-05,  3.35249642e-04, ...,\n",
       "          7.25874677e-04,  3.38114660e-05,  1.89146944e-04],\n",
       "        [ 3.10298812e-04,  1.17232441e-04,  7.13340298e-04, ...,\n",
       "          8.80761479e-04, -1.53829067e-04,  3.67903238e-04],\n",
       "        ...,\n",
       "        [ 8.68088100e-04,  5.84667025e-04, -1.06710917e-03, ...,\n",
       "         -3.63007101e-04, -3.98331409e-04,  1.94743072e-04],\n",
       "        [ 8.03169387e-04,  9.18795820e-04, -1.38724258e-03, ...,\n",
       "          3.03451379e-04, -8.19550827e-04,  4.62893309e-04],\n",
       "        [ 6.49140915e-04,  1.23413908e-03, -1.65102468e-03, ...,\n",
       "          1.01453438e-03, -1.22603495e-03,  6.77014992e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어보기\n",
    "model(src_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de40bca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델의 구조를 확인\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21ecf6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam() # Adam은 현재 가장 많이 사용하는 옵티마이저\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy( # 훈련 데이터의 라벨이 정수의 형태로 제공될 때 사용하는 손실함수\n",
    "    from_logits=True,\n",
    "    reduction='none' # 클래스 분류 문제에서 softmax 함수를 거치면 from_logits = False(default값),그렇지 않으면 from_logits = True.\n",
    ")\n",
    "# 모델을 학습시키키 위한 학습과정을 설정하는 단계\n",
    "model.compile(loss=loss, optimizer=optimizer) # 손실함수와 훈련과정을 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "645a8e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "609/609 [==============================] - 96s 151ms/step - loss: 3.4471\n",
      "Epoch 2/30\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 2.9594\n",
      "Epoch 3/30\n",
      "609/609 [==============================] - 99s 162ms/step - loss: 2.7828\n",
      "Epoch 4/30\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 2.6493\n",
      "Epoch 5/30\n",
      "609/609 [==============================] - 99s 162ms/step - loss: 2.5354\n",
      "Epoch 6/30\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 2.4321\n",
      "Epoch 7/30\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 2.3372\n",
      "Epoch 8/30\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 2.2495\n",
      "Epoch 9/30\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 2.1668\n",
      "Epoch 10/30\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 2.0888\n",
      "Epoch 11/30\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 2.0147\n",
      "Epoch 12/30\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 1.9437\n",
      "Epoch 13/30\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 1.8758\n",
      "Epoch 14/30\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 1.8094\n",
      "Epoch 15/30\n",
      "609/609 [==============================] - 100s 163ms/step - loss: 1.7467\n",
      "Epoch 16/30\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 1.6859\n",
      "Epoch 17/30\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 1.6276\n",
      "Epoch 18/30\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 1.5721\n",
      "Epoch 19/30\n",
      "609/609 [==============================] - 99s 162ms/step - loss: 1.5188\n",
      "Epoch 20/30\n",
      "609/609 [==============================] - 99s 162ms/step - loss: 1.4685\n",
      "Epoch 21/30\n",
      "609/609 [==============================] - 99s 162ms/step - loss: 1.4208\n",
      "Epoch 22/30\n",
      "609/609 [==============================] - 99s 162ms/step - loss: 1.3760\n",
      "Epoch 23/30\n",
      "609/609 [==============================] - 99s 162ms/step - loss: 1.3337\n",
      "Epoch 24/30\n",
      "609/609 [==============================] - 99s 162ms/step - loss: 1.2946\n",
      "Epoch 25/30\n",
      "609/609 [==============================] - 99s 162ms/step - loss: 1.2575\n",
      "Epoch 26/30\n",
      "609/609 [==============================] - 99s 162ms/step - loss: 1.2235\n",
      "Epoch 27/30\n",
      "609/609 [==============================] - 99s 162ms/step - loss: 1.1919\n",
      "Epoch 28/30\n",
      "609/609 [==============================] - 99s 162ms/step - loss: 1.1621\n",
      "Epoch 29/30\n",
      "609/609 [==============================] - 99s 162ms/step - loss: 1.1353\n",
      "Epoch 30/30\n",
      "609/609 [==============================] - 99s 162ms/step - loss: 1.1104\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(dataset, epochs=30)\n",
    "#Loss\n",
    "# tf.keras.losses.SparseCategoricalCrossentropy : https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy( \n",
    "    from_logits=True, reduction='none') # 클래스 분류 문제에서 softmax 함수를 거치면 from_logits = False(default값),그렇지 않으면 from_logits = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52738c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장생성 함수 정의\n",
    "#모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20): #시작 문자열을 init_sentence 로 받으며 디폴트값은 <start> 를 받는다\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence]) #텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다 (도달 하지 못하였으면 while 루프를 돌면서 다음 단어를 예측)\n",
    "    while True: #루프를 돌면서 init_sentence에 단어를 하나씩 생성\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4 \n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated #최종적으로 모델이 생성한 문장을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "836fde5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you so much <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20) # 시작문장으로 he를 넣어 문장생성 함수 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20ea72fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i am a god <end> '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i am\", max_len=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d154ba64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i like the way how you re touchin me <end> '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i like\", max_len=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24edec97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> my girl went through my cell phone <end> '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> my\", max_len=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94c65cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you re the only power <end> '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you\", max_len=20) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b34a9",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba23d739",
   "metadata": {},
   "source": [
    "- 이번 프로젝트에서 **어려웠던 점,**\n",
    "\n",
    "이전 fund에서 NLP에대해 처음접해보았는데, 개념부터 너무 어려웠었다.\n",
    "\n",
    "model fit을 하는데 시간이 너무 오래걸려서 힘들었다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa1aa89",
   "metadata": {},
   "source": [
    "- 프로젝트를 진행하면서 **알아낸 점** 혹은 **아직 모호한 점**.\n",
    "\n",
    "함수와 코드를 하나하나보면서 이해했어야했는데, 그러지 못한 상태로 마무리를 하게되어 아쉽다.\n",
    "\n",
    "자연어 처리를 위해서 정규표현식을 했는데, 정규표현식을 좀 더 공부해야겠다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00841827",
   "metadata": {},
   "source": [
    "- 루브릭 평가 지표를 맞추기 위해 **시도한 것들**.\n",
    "\n",
    "특수문자 제거, 토크나이저 생성, 토큰화 했을때 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외시키기, 단어장크기 12,000장 이상, 평가데이터셋 총 데이터의 20%, \n",
    "\n",
    "10 Epoch 안에 val_loss값 2.2 줄여보기\n",
    "\n",
    "Epoch 10/30\n",
    "609/609 [==============================] - 99s 163ms/step - loss: 2.0888\n",
    "\n",
    "텍스트 제너레이션 결과로 생성된 문장이 해석가능한 문장인지 살펴보기\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dec98bb",
   "metadata": {},
   "source": [
    "- 만약에 루브릭 평가 관련 지표를 **달성 하지 못했을 때, 이유에 관한 추정.느낌\n",
    "\n",
    "이전 노드의 내용을 그대로 참고해 학습부분에서 스스로 고민해본것이 부족한점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e49eb4",
   "metadata": {},
   "source": [
    "- **자기 다짐**\n",
    "\n",
    "관심있는 분야와 그렇지 않은 분야를 정해놓고 어느 한쪽에 집중하는건 맞지만 \n",
    "\n",
    "학습에서도 소홀히 하면 결국 나에게 좋을건 없다는걸 다시한번 더 깨닳았다.\n",
    "\n",
    "Embedding 개념을 다시 한번 살펴보고 이번 프로젝트와 19번 fund의 내용을 익힐 수 있게 노력해야겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec3aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
